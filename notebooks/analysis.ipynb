{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T10:28:01.906743Z",
     "start_time": "2025-08-01T10:28:01.902884Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T10:28:01.936575Z",
     "start_time": "2025-08-01T10:28:01.926995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wine Quality Classification - Comprehensive Analysis\n",
    "# This notebook provides detailed evaluation and analysis of machine learning models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.complete_workflow import *\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "print(\"üç∑ Wine Quality Classification - Analysis Notebook\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def load_latest_results(results_dir=\"results\", experiment_type=\"combined_comparison\"):\n",
    "    \"\"\"Load the most recent experiment results\"\"\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"‚ùå Results directory '{results_dir}' not found!\")\n",
    "        return None\n",
    "\n",
    "    # Find the latest file\n",
    "    files = [f for f in os.listdir(results_dir) if experiment_type in f and f.endswith('.pkl')]\n",
    "    if not files:\n",
    "        print(f\"‚ùå No {experiment_type} results found in {results_dir}\")\n",
    "        return None\n",
    "\n",
    "    latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(results_dir, x)))\n",
    "    file_path = os.path.join(results_dir, latest_file)\n",
    "\n",
    "    print(f\"üìÇ Loading results from: {latest_file}\")\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load experiment results\n",
    "results_data = load_latest_results()\n",
    "\n",
    "if results_data is None:\n",
    "    print(\"‚ö†Ô∏è  No results found. Please run main.py first to generate results.\")\n",
    "    print(\"   Then restart this notebook.\")\n",
    "else:\n",
    "    print(\"‚úÖ Results loaded successfully!\")\n",
    "\n",
    "    # Extract data\n",
    "    if 'results' in results_data and 'baseline' in results_data['results']:\n",
    "        # Combined results format\n",
    "        baseline_results = results_data['results']['baseline']\n",
    "        smote_results = results_data['results']['smote']\n",
    "        metadata = results_data['results'].get('metadata', {})\n",
    "    else:\n",
    "        # Single experiment format - try to load both baseline and smote\n",
    "        baseline_data = load_latest_results(experiment_type=\"baseline\")\n",
    "        smote_data = load_latest_results(experiment_type=\"smote_oversampling\")\n",
    "\n",
    "        if baseline_data and smote_data:\n",
    "            baseline_results = baseline_data['results']\n",
    "            smote_results = smote_data['results']\n",
    "            metadata = {}\n",
    "        else:\n",
    "            print(\"‚ùå Could not find both baseline and SMOTE results\")\n",
    "            baseline_results = smote_results = metadata = {}\n",
    "\n"
   ],
   "id": "6fa323728359e209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç∑ Wine Quality Classification - Analysis Notebook\n",
      "============================================================\n",
      "‚ùå Results directory 'results' not found!\n",
      "‚ö†Ô∏è  No results found. Please run main.py first to generate results.\n",
      "   Then restart this notebook.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T10:28:01.970505Z",
     "start_time": "2025-08-01T10:28:01.962301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 2. DATA OVERVIEW AND PREPROCESSING ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_data_overview():\n",
    "    \"\"\"Analyze the original dataset and preprocessing steps\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä DATA OVERVIEW & PREPROCESSING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load original data to analyze\n",
    "    try:\n",
    "        red_path = os.path.join(\"data\", \"winequality-red.csv\")\n",
    "        white_path = os.path.join(\"data\", \"winequality-white.csv\")\n",
    "\n",
    "        red_wine = pd.read_csv(red_path, sep=';')\n",
    "        white_wine = pd.read_csv(white_path, sep=';')\n",
    "\n",
    "        print(f\"üî¥ Red wine samples: {len(red_wine)}\")\n",
    "        print(f\"‚ö™ White wine samples: {len(white_wine)}\")\n",
    "        print(f\"üìà Total samples: {len(red_wine) + len(white_wine)}\")\n",
    "        print(f\"üî¢ Features: {len(red_wine.columns) - 1}\")  # -1 for quality column\n",
    "\n",
    "        # Combine data\n",
    "        red_wine['wine_type'] = 'red'\n",
    "        white_wine['wine_type'] = 'white'\n",
    "        combined_data = pd.concat([red_wine, white_wine], ignore_index=True)\n",
    "\n",
    "        # Create binary target\n",
    "        combined_data['quality_binary'] = (combined_data['quality'] >= 6).astype(int)\n",
    "\n",
    "        # Class distribution analysis\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Original quality distribution\n",
    "        combined_data['quality'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "        axes[0].set_title('Original Quality Distribution (3-9 scale)')\n",
    "        axes[0].set_xlabel('Quality Score')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "        # Binary quality distribution\n",
    "        binary_counts = combined_data['quality_binary'].value_counts()\n",
    "        axes[1].pie(binary_counts.values, labels=['Low Quality (< 6)', 'High Quality (‚â• 6)'],\n",
    "                   autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'lightgreen'])\n",
    "        axes[1].set_title('Binary Classification Target')\n",
    "\n",
    "        # Wine type distribution\n",
    "        type_counts = combined_data['wine_type'].value_counts()\n",
    "        axes[2].pie(type_counts.values, labels=type_counts.index.str.title(),\n",
    "                   autopct='%1.1f%%', startangle=90, colors=['darkred', 'gold'])\n",
    "        axes[2].set_title('Wine Type Distribution')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Class imbalance analysis\n",
    "        class_distribution = combined_data['quality_binary'].value_counts()\n",
    "        imbalance_ratio = class_distribution.max() / class_distribution.min()\n",
    "\n",
    "        print(f\"\\nüìä Class Distribution:\")\n",
    "        print(f\"   Low Quality (0): {class_distribution[0]:,} samples ({class_distribution[0]/len(combined_data)*100:.1f}%)\")\n",
    "        print(f\"   High Quality (1): {class_distribution[1]:,} samples ({class_distribution[1]/len(combined_data)*100:.1f}%)\")\n",
    "        print(f\"   Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "        if imbalance_ratio > 1.5:\n",
    "            print(f\"   ‚ö†Ô∏è  Dataset is imbalanced (ratio > 1.5:1)\")\n",
    "            print(f\"   üí° SMOTE oversampling should help address this\")\n",
    "\n",
    "        return combined_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Data files not found. Please ensure winequality-red.csv and winequality-white.csv are in the data/ directory.\")\n",
    "        return None\n",
    "\n",
    "# Run data analysis\n",
    "original_data = analyze_data_overview()\n",
    "\n"
   ],
   "id": "51588532a35afe96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä DATA OVERVIEW & PREPROCESSING ANALYSIS\n",
      "============================================================\n",
      "‚ùå Data files not found. Please ensure winequality-red.csv and winequality-white.csv are in the data/ directory.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T10:28:02.069570Z",
     "start_time": "2025-08-01T10:28:02.001851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 3. MODEL PERFORMANCE COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "def create_performance_comparison():\n",
    "    \"\"\"Create comprehensive performance comparison visualizations\"\"\"\n",
    "    if not baseline_results or not smote_results:\n",
    "        print(\"‚ùå Results not available for comparison\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Model name mapping for better display\n",
    "    model_names = {\n",
    "        'lr_scratch': 'Logistic Regression\\n(From Scratch)',\n",
    "        'lr_sklearn': 'Logistic Regression\\n(Scikit-learn)',\n",
    "        'svm_scratch': 'Linear SVM\\n(From Scratch)',\n",
    "        'svm_sklearn': 'Linear SVM\\n(Scikit-learn)',\n",
    "        'klr_scratch': 'Kernel Logistic\\nRegression',\n",
    "        'ksvm_scratch': 'Kernel SVM\\n(Pegasos)',\n",
    "        'rbf_svm_sklearn': 'RBF SVM\\n(Scikit-learn)',\n",
    "        'poly_svm_sklearn': 'Polynomial SVM\\n(Scikit-learn)'\n",
    "    }\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1']\n",
    "    metric_labels = ['Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "    # Find common models between baseline and SMOTE\n",
    "    common_models = set(baseline_results.keys()) & set(smote_results.keys())\n",
    "\n",
    "    if not common_models:\n",
    "        print(\"‚ùå No common models found between baseline and SMOTE results\")\n",
    "        return\n",
    "\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        models = []\n",
    "        baseline_scores = []\n",
    "        smote_scores = []\n",
    "\n",
    "        for model_key in sorted(common_models):\n",
    "            if model_key in baseline_results and model_key in smote_results:\n",
    "                models.append(model_names.get(model_key, model_key))\n",
    "                baseline_scores.append(baseline_results[model_key][metric])\n",
    "                smote_scores.append(smote_results[model_key][metric])\n",
    "\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "\n",
    "        bars1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline',\n",
    "                      color='lightcoral', alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, smote_scores, width, label='SMOTE',\n",
    "                      color='lightgreen', alpha=0.8)\n",
    "\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel(label)\n",
    "        ax.set_title(f'{label} Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # Remove empty subplot\n",
    "    axes[5].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Performance improvement analysis\n",
    "    print(\"\\nüìà PERFORMANCE IMPROVEMENTS (SMOTE vs Baseline):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    improvements = {}\n",
    "    for model_key in common_models:\n",
    "        model_name = model_names.get(model_key, model_key).replace('\\n', ' ')\n",
    "        improvements[model_name] = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            baseline_score = baseline_results[model_key][metric]\n",
    "            smote_score = smote_results[model_key][metric]\n",
    "            improvement = smote_score - baseline_score\n",
    "            improvements[model_name][metric] = improvement\n",
    "\n",
    "        # Print F1 improvements (most important for imbalanced data)\n",
    "        f1_improvement = improvements[model_name]['f1']\n",
    "        status = \"üìà\" if f1_improvement > 0.01 else \"üìâ\" if f1_improvement < -0.01 else \"‚ûñ\"\n",
    "        print(f\"{status} {model_name:<25}: F1 {f1_improvement:+.4f}\")\n",
    "\n",
    "    return improvements\n",
    "\n",
    "# Create performance comparison\n",
    "performance_improvements = create_performance_comparison()\n",
    "\n"
   ],
   "id": "feb6e28db6ccf71",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 111\u001B[39m\n\u001B[32m    108\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m improvements\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# Create performance comparison\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m performance_improvements = \u001B[43mcreate_performance_comparison\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mcreate_performance_comparison\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_performance_comparison\u001B[39m():\n\u001B[32m      6\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create comprehensive performance comparison visualizations\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mbaseline_results\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m smote_results:\n\u001B[32m      8\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m‚ùå Results not available for comparison\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'baseline_results' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 4. DETAILED METRICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_detailed_metrics():\n",
    "    \"\"\"Analyze metrics in detail with radar charts and heatmaps\"\"\"\n",
    "    if not baseline_results or not smote_results:\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç DETAILED METRICS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Create radar chart for top performing models\n",
    "    from math import pi\n",
    "\n",
    "    def create_radar_chart(results, title, ax):\n",
    "        \"\"\"Create radar chart for model performance\"\"\"\n",
    "        metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1']\n",
    "        metric_labels = ['Accuracy', 'Balanced\\nAccuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "        # Find top 4 models by F1 score\n",
    "        model_f1_scores = {model: results[model]['f1'] for model in results.keys()}\n",
    "        top_models = sorted(model_f1_scores.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "\n",
    "        # Number of variables\n",
    "        categories = metric_labels\n",
    "        N = len(categories)\n",
    "\n",
    "        # Compute angle for each axis\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "\n",
    "        colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "        for i, (model_key, _) in enumerate(top_models):\n",
    "            values = [results[model_key][metric] for metric in metrics]\n",
    "            values += values[:1]  # Complete the circle\n",
    "\n",
    "            model_name = model_key.replace('_', ' ').title()\n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "            ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(title, size=14, fontweight='bold')\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Create radar charts\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "    create_radar_chart(baseline_results, 'Baseline Performance', ax1)\n",
    "    create_radar_chart(smote_results, 'SMOTE Performance', ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Metrics correlation heatmap\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    def create_metrics_heatmap(results, title, ax):\n",
    "        \"\"\"Create correlation heatmap of metrics across models\"\"\"\n",
    "        metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "        # Create dataframe with metrics for each model\n",
    "        data = []\n",
    "        for model in results.keys():\n",
    "            row = [results[model][metric] for metric in metrics]\n",
    "            data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(data,\n",
    "                         columns=['Accuracy', 'Bal. Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "                         index=list(results.keys()))\n",
    "\n",
    "        # Compute correlation matrix\n",
    "        corr_matrix = df.corr()\n",
    "\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "        ax.set_title(f'{title} - Metrics Correlation')\n",
    "\n",
    "    create_metrics_heatmap(baseline_results, 'Baseline', ax1)\n",
    "    create_metrics_heatmap(smote_results, 'SMOTE', ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run detailed metrics analysis\n",
    "analyze_detailed_metrics()\n",
    "\n"
   ],
   "id": "f6893c8d51c65b37",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 5. MODEL ARCHITECTURE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_model_architectures():\n",
    "    \"\"\"Analyze different model architectures and their characteristics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèóÔ∏è MODEL ARCHITECTURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Group models by type\n",
    "    model_groups = {\n",
    "        'Linear Models': ['lr_scratch', 'lr_sklearn', 'svm_scratch', 'svm_sklearn'],\n",
    "        'Kernel Methods': ['klr_scratch', 'ksvm_scratch', 'rbf_svm_sklearn', 'poly_svm_sklearn']\n",
    "    }\n",
    "\n",
    "    if not baseline_results:\n",
    "        return\n",
    "\n",
    "    # Analyze performance by model type\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    for i, (group_name, models) in enumerate(model_groups.items()):\n",
    "        row = i\n",
    "\n",
    "        # Filter models that exist in results\n",
    "        existing_models = [m for m in models if m in baseline_results]\n",
    "        if not existing_models:\n",
    "            continue\n",
    "\n",
    "        # F1 scores comparison\n",
    "        ax1 = axes[row, 0]\n",
    "        baseline_f1 = [baseline_results[m]['f1'] for m in existing_models if m in baseline_results]\n",
    "        smote_f1 = [smote_results[m]['f1'] for m in existing_models if m in smote_results]\n",
    "\n",
    "        x = np.arange(len(existing_models))\n",
    "        width = 0.35\n",
    "\n",
    "        ax1.bar(x - width/2, baseline_f1, width, label='Baseline', color='lightcoral', alpha=0.8)\n",
    "        ax1.bar(x + width/2, smote_f1, width, label='SMOTE', color='lightgreen', alpha=0.8)\n",
    "\n",
    "        ax1.set_title(f'{group_name} - F1 Score Comparison')\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels([m.replace('_', '\\n') for m in existing_models], rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Balanced accuracy comparison\n",
    "        ax2 = axes[row, 1]\n",
    "        baseline_bal_acc = [baseline_results[m]['balanced_accuracy'] for m in existing_models if m in baseline_results]\n",
    "        smote_bal_acc = [smote_results[m]['balanced_accuracy'] for m in existing_models if m in smote_results]\n",
    "\n",
    "        ax2.bar(x - width/2, baseline_bal_acc, width, label='Baseline', color='lightcoral', alpha=0.8)\n",
    "        ax2.bar(x + width/2, smote_bal_acc, width, label='SMOTE', color='lightgreen', alpha=0.8)\n",
    "\n",
    "        ax2.set_title(f'{group_name} - Balanced Accuracy Comparison')\n",
    "        ax2.set_xlabel('Accuracy')\n",
    "        ax2.set_ylabel('Balanced Accuracy')\n",
    "        ax2.set_title('SMOTE: Accuracy vs Balanced Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add model labels\n",
    "        for i, model in enumerate(smote_models):\n",
    "            ax2.annotate(model.replace('_', '\\n'),\n",
    "                        (smote_accuracy_scores[i], smote_balanced_accuracy_scores[i]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    # 2. Precision vs Recall analysis (indicates class bias issues)\n",
    "    ax3 = axes[1, 0]\n",
    "    precision_scores = [baseline_results[m]['precision'] for m in models]\n",
    "    recall_scores = [baseline_results[m]['recall'] for m in models]\n",
    "\n",
    "    ax3.scatter(precision_scores, recall_scores, alpha=0.7, s=100)\n",
    "    ax3.set_xlabel('Precision')\n",
    "    ax3.set_ylabel('Recall')\n",
    "    ax3.set_title('Baseline: Precision vs Recall Trade-off')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add model labels\n",
    "    for i, model in enumerate(models):\n",
    "        ax3.annotate(model.replace('_', '\\n'),\n",
    "                    (precision_scores[i], recall_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    # SMOTE Precision vs Recall\n",
    "    if smote_results:\n",
    "        ax4 = axes[1, 1]\n",
    "        smote_precision_scores = [smote_results[m]['precision'] for m in smote_models]\n",
    "        smote_recall_scores = [smote_results[m]['recall'] for m in smote_models]\n",
    "\n",
    "        ax4.scatter(smote_precision_scores, smote_recall_scores, alpha=0.7, s=100, color='green')\n",
    "        ax4.set_xlabel('Precision')\n",
    "        ax4.set_ylabel('Recall')\n",
    "        ax4.set_title('SMOTE: Precision vs Recall Trade-off')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add model labels\n",
    "        for i, model in enumerate(smote_models):\n",
    "            ax4.annotate(model.replace('_', '\\n'),\n",
    "                        (smote_precision_scores[i], smote_recall_scores[i]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Fitting behavior analysis\n",
    "    print(\"üîç FITTING BEHAVIOR ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Identify potential overfitting/underfitting patterns\n",
    "    for model in models:\n",
    "        if model not in baseline_results:\n",
    "            continue\n",
    "\n",
    "        accuracy = baseline_results[model]['accuracy']\n",
    "        balanced_accuracy = baseline_results[model]['balanced_accuracy']\n",
    "        precision = baseline_results[model]['precision']\n",
    "        recall = baseline_results[model]['recall']\n",
    "        f1 = baseline_results[model]['f1']\n",
    "\n",
    "        # Analysis criteria\n",
    "        acc_bal_diff = accuracy - balanced_accuracy\n",
    "        precision_recall_diff = abs(precision - recall)\n",
    "\n",
    "        print(f\"\\nüìä {model.replace('_', ' ').title()}:\")\n",
    "\n",
    "        # Overfitting indicators\n",
    "        if acc_bal_diff > 0.05:\n",
    "            print(f\"   ‚ö†Ô∏è  Potential overfitting: Accuracy ({accuracy:.3f}) >> Balanced Accuracy ({balanced_accuracy:.3f})\")\n",
    "            print(f\"      ‚Üí Model may be biased toward majority class\")\n",
    "\n",
    "        # Class imbalance handling\n",
    "        if precision_recall_diff > 0.1:\n",
    "            if precision > recall:\n",
    "                print(f\"   üìà High precision ({precision:.3f}), lower recall ({recall:.3f})\")\n",
    "                print(f\"      ‚Üí Conservative predictions (few false positives)\")\n",
    "            else:\n",
    "                print(f\"   üìà High recall ({recall:.3f}), lower precision ({precision:.3f})\")\n",
    "                print(f\"      ‚Üí Liberal predictions (few false negatives)\")\n",
    "\n",
    "        # Overall performance assessment\n",
    "        if f1 < 0.7:\n",
    "            print(f\"   üìâ Low F1-score ({f1:.3f}) suggests underfitting\")\n",
    "            print(f\"      ‚Üí Consider more complex model or better features\")\n",
    "        elif f1 > 0.85:\n",
    "            print(f\"   üéØ Excellent F1-score ({f1:.3f}) - well-fitted model\")\n",
    "\n",
    "        # SMOTE improvement analysis\n",
    "        if model in smote_results:\n",
    "            smote_f1 = smote_results[model]['f1']\n",
    "            smote_bal_acc = smote_results[model]['balanced_accuracy']\n",
    "\n",
    "            f1_improvement = smote_f1 - f1\n",
    "            bal_acc_improvement = smote_bal_acc - balanced_accuracy\n",
    "\n",
    "            if f1_improvement > 0.02:\n",
    "                print(f\"   ‚úÖ SMOTE significantly improved F1 (+{f1_improvement:.3f})\")\n",
    "                print(f\"      ‚Üí Original model suffered from class imbalance\")\n",
    "            elif f1_improvement < -0.02:\n",
    "                print(f\"   ‚ùå SMOTE hurt performance ({f1_improvement:.3f})\")\n",
    "                print(f\"      ‚Üí Model was already handling imbalance well\")\n",
    "\n"
   ],
   "id": "db6a3bc779572e04",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 7. MISCLASSIFICATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_misclassifications():\n",
    "    \"\"\"Analyze patterns in misclassified examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç MISCLASSIFICATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Note: This is a conceptual analysis since we don't have access to individual predictions\n",
    "    # In a real scenario, you would retrain models and capture prediction details\n",
    "\n",
    "    print(\"üìã MISCLASSIFICATION PATTERNS (Conceptual Analysis):\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if not baseline_results:\n",
    "        return\n",
    "\n",
    "    # Analyze performance patterns to infer misclassification issues\n",
    "    models_analysis = {}\n",
    "\n",
    "    for model in baseline_results.keys():\n",
    "        accuracy = baseline_results[model]['accuracy']\n",
    "        balanced_accuracy = baseline_results[model]['balanced_accuracy']\n",
    "        precision = baseline_results[model]['precision']\n",
    "        recall = baseline_results[model]['recall']\n",
    "\n",
    "        analysis = {\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'accuracy_gap': accuracy - balanced_accuracy,\n",
    "            'precision_recall_ratio': precision / recall if recall > 0 else 0\n",
    "        }\n",
    "\n",
    "        models_analysis[model] = analysis\n",
    "\n",
    "    # Create visualization of misclassification patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # 1. Accuracy vs Balanced Accuracy Gap (indicates majority class bias)\n",
    "    ax1 = axes[0, 0]\n",
    "    models = list(models_analysis.keys())\n",
    "    accuracy_gaps = [models_analysis[m]['accuracy_gap'] for m in models]\n",
    "\n",
    "    bars = ax1.bar(range(len(models)), accuracy_gaps, color=['red' if gap > 0.03 else 'orange' if gap > 0.01 else 'green' for gap in accuracy_gaps])\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Accuracy - Balanced Accuracy')\n",
    "    ax1.set_title('Majority Class Bias Indicator')\n",
    "    ax1.set_xticks(range(len(models)))\n",
    "    ax1.set_xticklabels([m.replace('_', '\\n') for m in models], rotation=45)\n",
    "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax1.axhline(y=0.03, color='red', linestyle='--', alpha=0.5, label='High Bias Threshold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Precision/Recall Ratio (indicates prediction tendency)\n",
    "    ax2 = axes[0, 1]\n",
    "    pr_ratios = [models_analysis[m]['precision_recall_ratio'] for m in models]\n",
    "\n",
    "    bars = ax2.bar(range(len(models)), pr_ratios,\n",
    "                   color=['blue' if ratio > 1.2 else 'purple' if ratio < 0.8 else 'green' for ratio in pr_ratios])\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('Precision / Recall Ratio')\n",
    "    ax2.set_title('Prediction Tendency Analysis')\n",
    "    ax2.set_xticks(range(len(models)))\n",
    "    ax2.set_xticklabels([m.replace('_', '\\n') for m in models], rotation=45)\n",
    "    ax2.axhline(y=1, color='black', linestyle='-', alpha=0.3, label='Perfect Balance')\n",
    "    ax2.axhline(y=1.2, color='blue', linestyle='--', alpha=0.5, label='Conservative Threshold')\n",
    "    ax2.axhline(y=0.8, color='purple', linestyle='--', alpha=0.5, label='Liberal Threshold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Model Complexity vs Performance (overfitting indicator)\n",
    "    ax3 = axes[1, 0]\n",
    "\n",
    "    # Assign complexity scores (conceptual)\n",
    "    complexity_scores = {\n",
    "        'lr_scratch': 1, 'lr_sklearn': 1,\n",
    "        'svm_scratch': 2, 'svm_sklearn': 2,\n",
    "        'klr_scratch': 4, 'ksvm_scratch': 4,\n",
    "        'rbf_svm_sklearn': 3, 'poly_svm_sklearn': 3\n",
    "    }\n",
    "\n",
    "    complexities = [complexity_scores.get(m, 2) for m in models]\n",
    "    f1_scores = [baseline_results[m]['f1'] for m in models]\n",
    "\n",
    "    colors = ['red', 'orange', 'yellow', 'green', 'blue']\n",
    "    ax3.scatter(complexities, f1_scores, c=[colors[min(c-1, 4)] for c in complexities], s=100, alpha=0.7)\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        ax3.annotate(model.replace('_', '\\n'),\n",
    "                    (complexities[i], f1_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    ax3.set_xlabel('Model Complexity (1=Linear, 4=Kernel)')\n",
    "    ax3.set_ylabel('F1 Score')\n",
    "    ax3.set_title('Complexity vs Performance Trade-off')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. SMOTE Impact Analysis\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    if smote_results:\n",
    "        baseline_f1 = [baseline_results[m]['f1'] for m in models if m in smote_results]\n",
    "        smote_f1 = [smote_results[m]['f1'] for m in models if m in smote_results]\n",
    "        improvements = [s - b for b, s in zip(baseline_f1, smote_f1)]\n",
    "\n",
    "        bars = ax4.bar(range(len(improvements)), improvements,\n",
    "                       color=['green' if imp > 0.01 else 'red' if imp < -0.01 else 'yellow' for imp in improvements])\n",
    "\n",
    "        ax4.set_xlabel('Models')\n",
    "        ax4.set_ylabel('F1 Score Improvement (SMOTE - Baseline)')\n",
    "        ax4.set_title('SMOTE Impact on Performance')\n",
    "        ax4.set_xticks(range(len(improvements)))\n",
    "        ax4.set_xticklabels([m.replace('_', '\\n') for m in models if m in smote_results], rotation=45)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed misclassification insights\n",
    "    print(\"\\nüîç MISCLASSIFICATION INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nüìä {model.replace('_', ' ').title()}:\")\n",
    "\n",
    "        gap = models_analysis[model]['accuracy_gap']\n",
    "        pr_ratio = models_analysis[model]['precision_recall_ratio']\n",
    "\n",
    "        # Bias analysis\n",
    "        if gap > 0.05:\n",
    "            print(f\"   üî¥ HIGH BIAS: Likely many false negatives (missing high-quality wines)\")\n",
    "            print(f\"      ‚Üí Model over-predicts majority class (low quality)\")\n",
    "        elif gap > 0.02:\n",
    "            print(f\"   üü° MODERATE BIAS: Some tendency toward majority class\")\n",
    "        else:\n",
    "            print(f\"   üü¢ LOW BIAS: Well-balanced predictions\")\n",
    "\n",
    "        # Prediction tendency analysis\n",
    "        if pr_ratio > 1.3:\n",
    "            print(f\"   üîµ CONSERVATIVE: High precision, low recall\")\n",
    "            print(f\"      ‚Üí Few false positives, but misses many true positives\")\n",
    "            print(f\"      ‚Üí Misclassifies many high-quality wines as low-quality\")\n",
    "        elif pr_ratio < 0.7:\n",
    "            print(f\"   üü£ LIBERAL: Low precision, high recall\")\n",
    "            print(f\"      ‚Üí Few false negatives, but many false positives\")\n",
    "            print(f\"      ‚Üí Misclassifies many low-quality wines as high-quality\")\n",
    "        else:\n",
    "            print(f\"   üü¢ BALANCED: Good precision-recall trade-off\")\n",
    "\n",
    "        # SMOTE impact\n",
    "        if model in smote_results:\n",
    "            smote_f1 = smote_results[model]['f1']\n",
    "            baseline_f1 = baseline_results[model]['f1']\n",
    "            improvement = smote_f1 - baseline_f1\n",
    "\n",
    "            if improvement > 0.02:\n",
    "                print(f\"   ‚úÖ SMOTE HELPED: Reduced class imbalance misclassifications\")\n",
    "            elif improvement < -0.02:\n",
    "                print(f\"   ‚ùå SMOTE HURT: May have introduced noise or overfitting\")\n",
    "            else:\n",
    "                print(f\"   ‚ûñ SMOTE NEUTRAL: Minimal impact on misclassifications\")\n",
    "\n",
    "# Run misclassification analysis\n",
    "analyze_misclassifications()\n",
    "\n"
   ],
   "id": "45ec5cec200ee314",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 8. FEATURE IMPORTANCE AND MODEL INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_insights():\n",
    "    \"\"\"Analyze feature importance and model behavior insights\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ FEATURE IMPORTANCE & MODEL INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load and analyze original data for feature insights\n",
    "    try:\n",
    "        if original_data is not None:\n",
    "            # Correlation analysis\n",
    "            features = original_data.select_dtypes(include=[np.number]).columns\n",
    "            features = [f for f in features if f not in ['quality', 'quality_binary']]\n",
    "\n",
    "            correlation_matrix = original_data[features + ['quality_binary']].corr()\n",
    "            target_correlations = correlation_matrix['quality_binary'].drop('quality_binary').abs().sort_values(ascending=False)\n",
    "\n",
    "            # Visualize feature correlations with target\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "            # Feature correlation with target\n",
    "            ax1 = axes[0, 0]\n",
    "            target_correlations.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "            ax1.set_title('Feature Correlation with Wine Quality (Binary)')\n",
    "            ax1.set_xlabel('Features')\n",
    "            ax1.set_ylabel('Absolute Correlation')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Feature correlation heatmap\n",
    "            ax2 = axes[0, 1]\n",
    "            top_features = target_correlations.head(8).index.tolist()\n",
    "            subset_corr = original_data[top_features + ['quality_binary']].corr()\n",
    "\n",
    "            sns.heatmap(subset_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                       square=True, ax=ax2, cbar_kws={'shrink': 0.8})\n",
    "            ax2.set_title('Top Features Correlation Matrix')\n",
    "\n",
    "            # Distribution of top features by quality\n",
    "            ax3 = axes[1, 0]\n",
    "            top_feature = target_correlations.index[0]\n",
    "\n",
    "            original_data.boxplot(column=top_feature, by='quality_binary', ax=ax3)\n",
    "            ax3.set_title(f'{top_feature} Distribution by Quality')\n",
    "            ax3.set_xlabel('Quality (0=Low, 1=High)')\n",
    "            ax3.set_ylabel(top_feature)\n",
    "\n",
    "            # Feature importance ranking\n",
    "            ax4 = axes[1, 1]\n",
    "            importance_data = target_correlations.head(10)\n",
    "\n",
    "            bars = ax4.barh(range(len(importance_data)), importance_data.values,\n",
    "                           color='lightgreen', alpha=0.7)\n",
    "            ax4.set_yticks(range(len(importance_data)))\n",
    "            ax4.set_yticklabels(importance_data.index)\n",
    "            ax4.set_xlabel('Correlation with Quality')\n",
    "            ax4.set_title('Top 10 Most Important Features')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Print feature insights\n",
    "            print(f\"üîç TOP PREDICTIVE FEATURES:\")\n",
    "            print(\"-\" * 30)\n",
    "            for i, (feature, corr) in enumerate(target_correlations.head(5).items(), 1):\n",
    "                print(f\"{i}. {feature}: {corr:.3f} correlation\")\n",
    "\n",
    "            print(f\"\\nüí° FEATURE INSIGHTS:\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "            # Analyze top features\n",
    "            top_3_features = target_correlations.head(3).index.tolist()\n",
    "            for feature in top_3_features:\n",
    "                corr = target_correlations[feature]\n",
    "\n",
    "                high_quality_mean = original_data[original_data['quality_binary'] == 1][feature].mean()\n",
    "                low_quality_mean = original_data[original_data['quality_binary'] == 0][feature].mean()\n",
    "\n",
    "                if corr > 0:\n",
    "                    print(f\"üìà {feature}: Higher values ‚Üí Higher quality\")\n",
    "                    print(f\"   High quality avg: {high_quality_mean:.3f}\")\n",
    "                    print(f\"   Low quality avg: {low_quality_mean:.3f}\")\n",
    "                else:\n",
    "                    print(f\"üìâ {feature}: Higher values ‚Üí Lower quality\")\n",
    "                    print(f\"   High quality avg: {high_quality_mean:.3f}\")\n",
    "                    print(f\"   Low quality avg: {low_quality_mean:.3f}\")\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Original data not available for feature analysis\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in feature analysis: {e}\")\n",
    "\n",
    "# Run feature insights analysis\n",
    "analyze_feature_insights()\n",
    "\n"
   ],
   "id": "bd2019582b8a102c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 9. FINAL RECOMMENDATIONS AND CONCLUSIONS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_final_recommendations():\n",
    "    \"\"\"Generate final recommendations based on all analyses\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã FINAL RECOMMENDATIONS & CONCLUSIONS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if not baseline_results or not smote_results:\n",
    "        print(\"‚ùå Cannot generate recommendations without complete results\")\n",
    "        return\n",
    "\n",
    "    # Find best performing models\n",
    "    baseline_best = max(baseline_results.items(), key=lambda x: x[1]['f1'])\n",
    "    smote_best = max(smote_results.items(), key=lambda x: x[1]['f1'])\n",
    "\n",
    "    print(f\"üèÜ BEST PERFORMING MODELS:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Baseline: {baseline_best[0]} (F1: {baseline_best[1]['f1']:.4f})\")\n",
    "    print(f\"SMOTE: {smote_best[0]} (F1: {smote_best[1]['f1']:.4f})\")\n",
    "\n",
    "    # Overall recommendations\n",
    "    print(f\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # 1. SMOTE effectiveness\n",
    "    smote_improvements = []\n",
    "    for model in baseline_results.keys():\n",
    "        if model in smote_results:\n",
    "            improvement = smote_results[model]['f1'] - baseline_results[model]['f1']\n",
    "            smote_improvements.append(improvement)\n",
    "\n",
    "    avg_smote_improvement = np.mean(smote_improvements) if smote_improvements else 0\n",
    "\n",
    "    if avg_smote_improvement > 0.01:\n",
    "        print(f\"‚úÖ 1. USE SMOTE: Average F1 improvement of {avg_smote_improvement:.3f}\")\n",
    "        print(f\"      SMOTE effectively addresses class imbalance in this dataset\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  1. SMOTE IMPACT: Minimal average improvement ({avg_smote_improvement:.3f})\")\n",
    "        print(f\"      Consider other rebalancing techniques or feature engineering\")\n",
    "\n",
    "    # 2. Model architecture recommendations\n",
    "    kernel_models = [k for k in baseline_results.keys() if 'kernel' in k or 'rbf' in k or 'poly' in k]\n",
    "    linear_models = [k for k in baseline_results.keys() if k not in kernel_models]\n",
    "\n",
    "    if kernel_models and linear_models:\n",
    "        kernel_avg_f1 = np.mean([baseline_results[m]['f1'] for m in kernel_models])\n",
    "        linear_avg_f1 = np.mean([baseline_results[m]['f1'] for m in linear_models])\n",
    "\n",
    "        if kernel_avg_f1 > linear_avg_f1 + 0.02:\n",
    "            print(f\"‚úÖ 2. USE KERNEL METHODS: {kernel_avg_f1:.3f} vs {linear_avg_f1:.3f} avg F1\")\n",
    "            print(f\"      Non-linear relationships are important in this dataset\")\n",
    "        else:\n",
    "            print(f\"‚úÖ 2. LINEAR MODELS SUFFICIENT: Comparable performance to kernel methods\")\n",
    "            print(f\"      Consider linear models for faster training and better interpretability\")\n",
    "\n",
    "    # 3. Implementation recommendations\n",
    "    scratch_models = [k for k in baseline_results.keys() if 'scratch' in k]\n",
    "    sklearn_models = [k for k in baseline_results.keys() if 'sklearn' in k]\n",
    "\n",
    "    if scratch_models and sklearn_models:\n",
    "        scratch_avg_f1 = np.mean([baseline_results[m]['f1'] for m in scratch_models])\n",
    "        sklearn_avg_f1 = np.mean([baseline_results[m]['f1'] for m in sklearn_models])\n",
    "\n",
    "        if abs(scratch_avg_f1 - sklearn_avg_f1) < 0.02:\n",
    "            print(f\"‚úÖ 3. IMPLEMENTATION QUALITY: Custom implementations work well\")\n",
    "            print(f\"      From-scratch: {scratch_avg_f1:.3f}, Scikit-learn: {sklearn_avg_f1:.3f}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  3. IMPLEMENTATION GAP: {abs(scratch_avg_f1 - sklearn_avg_f1):.3f} difference\")\n",
    "            print(f\"      Consider optimizing custom implementations\")\n",
    "\n",
    "    # 4. Overfitting analysis\n",
    "    high_variance_models = []\n",
    "    for model in baseline_results.keys():\n",
    "        acc_gap = baseline_results[model]['accuracy'] - baseline_results[model]['balanced_accuracy']\n",
    "        if acc_gap > 0.03:\n",
    "            high_variance_models.append(model)\n",
    "\n",
    "    if high_variance_models:\n",
    "        print(f\"‚ö†Ô∏è  4. OVERFITTING DETECTED in: {', '.join(high_variance_models)}\")\n",
    "        print(f\"      Consider regularization or cross-validation tuning\")\n",
    "    else:\n",
    "        print(f\"‚úÖ 4. NO MAJOR OVERFITTING: Models show good generalization\")\n",
    "\n",
    "    # 5. Production recommendations\n",
    "    print(f\"\\nüöÄ PRODUCTION RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    # Balance performance vs complexity\n",
    "    simple_models = ['lr_scratch', 'lr_sklearn', 'svm_scratch', 'svm_sklearn']\n",
    "    complex_models = [k for k in baseline_results.keys() if k not in simple_models]\n",
    "\n",
    "    best_simple = max([(k, v) for k, v in smote_results.items() if k in simple_models],\n",
    "                     key=lambda x: x[1]['f1'], default=(None, None))\n",
    "    best_complex = max([(k, v) for k, v in smote_results.items() if k in complex_models],\n",
    "                      key=lambda x: x[1]['f1'], default=(None, None))\n",
    "\n",
    "    if best_simple[0] and best_complex[0]:\n",
    "        simple_f1 = best_simple[1]['f1']\n",
    "        complex_f1 = best_complex[1]['f1']\n",
    "\n",
    "        if complex_f1 > simple_f1 + 0.03:\n",
    "            print(f\"üéØ RECOMMENDED: {best_complex[0]} (F1: {complex_f1:.4f})\")\n",
    "            print(f\"   Performance gain ({complex_f1 - simple_f1:.3f}) justifies complexity\")\n",
    "        else:\n",
    "            print(f\"üéØ RECOMMENDED: {best_simple[0]} (F1: {simple_f1:.4f})\")\n",
    "            print(f\"   Simpler model with comparable performance ({simple_f1:.4f} vs {complex_f1:.4f})\")\n",
    "\n",
    "    # Model deployment considerations\n",
    "    print(f\"\\n‚öôÔ∏è  DEPLOYMENT CONSIDERATIONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚Ä¢ Data preprocessing: Standardization + log transformation essential\")\n",
    "    print(f\"‚Ä¢ Class imbalance: Monitor precision/recall in production\")\n",
    "    print(f\"‚Ä¢ Feature monitoring: Track {original_data.select_dtypes(include=[np.number]).columns.tolist()[:3] if original_data is not None else 'key features'}\")\n",
    "    print(f\"‚Ä¢ Model retraining: Consider when class distribution shifts\")\n",
    "\n",
    "    return {\n",
    "        'best_baseline': baseline_best,\n",
    "        'best_smote': smote_best,\n",
    "        'smote_improvement': avg_smote_improvement,\n",
    "        'recommendations': 'Generated successfully'\n",
    "    }\n",
    "\n",
    "# Generate final recommendations\n",
    "final_recommendations = generate_final_recommendations()\n",
    "\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"This comprehensive analysis covers:\")\n",
    "print(\"‚úÖ Data overview and preprocessing analysis\")\n",
    "print(\"‚úÖ Model performance comparison\")\n",
    "print(\"‚úÖ Detailed metrics analysis with visualizations\")\n",
    "print(\"‚úÖ Model architecture comparison\")\n",
    "print(\"‚úÖ Overfitting/underfitting analysis\")\n",
    "print(\"‚úÖ Misclassification pattern analysis\")\n",
    "print(\"‚úÖ Feature importance insights\")\n",
    "print(\"‚úÖ Final recommendations for production\")\n",
    "print(f\"\\nAll visualizations and metrics provide insights for model selection and deployment decisions.\")('Models')\n",
    "        ax2.set_ylabel('Balanced Accuracy')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([m.replace('_', '\\n') for m in existing_models], rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Architecture insights\n",
    "    print(\"üîç ARCHITECTURE INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Compare scratch vs sklearn implementations\n",
    "    scratch_models = [k for k in baseline_results.keys() if 'scratch' in k]\n",
    "    sklearn_models = [k for k in baseline_results.keys() if 'sklearn' in k]\n",
    "\n",
    "    if scratch_models and sklearn_models:\n",
    "        print(\"\\nüìä Implementation Comparison (From Scratch vs. Scikit-learn):\")\n",
    "\n",
    "        scratch_avg_f1 = np.mean([baseline_results[m]['f1'] for m in scratch_models])\n",
    "        sklearn_avg_f1 = np.mean([baseline_results[m]['f1'] for m in sklearn_models])\n",
    "\n",
    "        print(f\"   From Scratch Average F1: {scratch_avg_f1:.4f}\")\n",
    "        print(f\"   Scikit-learn Average F1: {sklearn_avg_f1:.4f}\")\n",
    "        print(f\"   Difference: {sklearn_avg_f1 - scratch_avg_f1:+.4f}\")\n",
    "\n",
    "        if abs(sklearn_avg_f1 - scratch_avg_f1) < 0.02:\n",
    "            print(\"   ‚úÖ Custom implementations perform comparably to scikit-learn\")\n",
    "        elif scratch_avg_f1 > sklearn_avg_f1:\n",
    "            print(\"   üöÄ Custom implementations outperform scikit-learn!\")\n",
    "        else:\n",
    "            print(\"   üìö Scikit-learn implementations are more optimized\")\n",
    "\n",
    "# Run architecture analysis\n",
    "analyze_model_architectures()\n",
    "\n"
   ],
   "id": "acb1c54b6d95afb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 6. OVERFITTING/UNDERFITTING ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_fitting_behavior():\n",
    "    \"\"\"Analyze overfitting and underfitting patterns\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà OVERFITTING/UNDERFITTING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if not baseline_results:\n",
    "        return\n",
    "\n",
    "    # Analyze performance patterns that indicate overfitting/underfitting\n",
    "\n",
    "    # 1. Compare accuracy vs balanced accuracy (balanced accuracy drops more with overfitting)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Baseline analysis\n",
    "    models = list(baseline_results.keys())\n",
    "    accuracy_scores = [baseline_results[m]['accuracy'] for m in models]\n",
    "    balanced_accuracy_scores = [baseline_results[m]['balanced_accuracy'] for m in models]\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(accuracy_scores, balanced_accuracy_scores, alpha=0.7, s=100)\n",
    "\n",
    "    # Add diagonal line (perfect balance)\n",
    "    min_score = min(min(accuracy_scores), min(balanced_accuracy_scores))\n",
    "    max_score = max(max(accuracy_scores), max(balanced_accuracy_scores))\n",
    "    ax1.plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.5, label='Perfect Balance')\n",
    "\n",
    "    ax1.set_xlabel('Accuracy')\n",
    "    ax1.set_ylabel('Balanced Accuracy')\n",
    "    ax1.set_title('Baseline: Accuracy vs Balanced Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add model labels\n",
    "    for i, model in enumerate(models):\n",
    "        ax1.annotate(model.replace('_', '\\n'),\n",
    "                    (accuracy_scores[i], balanced_accuracy_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    # SMOTE analysis\n",
    "    if smote_results:\n",
    "        smote_accuracy_scores = [smote_results[m]['accuracy'] for m in models if m in smote_results]\n",
    "        smote_balanced_accuracy_scores = [smote_results[m]['balanced_accuracy'] for m in models if m in smote_results]\n",
    "        smote_models = [m for m in models if m in smote_results]\n",
    "\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.scatter(smote_accuracy_scores, smote_balanced_accuracy_scores, alpha=0.7, s=100, color='green')\n",
    "\n",
    "        min_score = min(min(smote_accuracy_scores), min(smote_balanced_accuracy_scores))\n",
    "        max_score = max(max(smote_accuracy_scores), max(smote_balanced_accuracy_scores))\n",
    "        ax2.plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.5, label='Perfect Balance')\n",
    "\n",
    "        ax2.set_xlabel\n",
    "\n",
    "# Run fitting behavior analysis\n",
    "analyze_fitting_behavior()"
   ],
   "id": "1e054ac01e106f49",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
